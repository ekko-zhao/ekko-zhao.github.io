---
title: 安全客漏洞列表爬虫
date: 2018-07-02 13:43:40
tags: "python"
---

逛安全客发现他竟然还有漏洞列表的版块，翻了一下，还挺齐全的。于是写了个爬虫练练手，在此记录一哈。

代码分为2个部分，第一部分列出所有漏洞，包括漏洞名称，CVE编号，发布时间等信息，第二部分将漏洞列表中的信息写入表格并保存。

先贴上主要代码：
``` python
def getVulnInfo(url):
	vulns = []
	html = readHtml(url)
	soup = BeautifulSoup(html,'html.parser')
	
	for item  in soup.select('tbody tr'):
		vuln_url = 'https://www.anquanke.com' + item.select('.vul-title-item a')[0]['href']
		vuln_name = item.select('.vul-title-item a')[0].text.replace('\xa9','@@').replace('\u0130','@@').replace('\xae','@@') #哎，特殊符号一直报错，直接替换掉了
		try:
			cve = item.select('.vul-cve-item')[0].text
		except:
			cve = 'N/A'
		start_time = item.select('.vul-date-item')[0].text.strip()
		update_time = item.select('.vul-date-item')[1].text.strip()
		vuln = [vuln_url,vuln_name,cve,start_time,update_time]
		vulns.append(vuln)
	return vulns
```
定义此函数获取每个漏洞的基本信息，这里遇到个小坑，开始写完的时候运行一直报错：
``` python
UnicodeEncodeError: 'gbk' codec can't encode character '\xae' in position 21: illegal multibyte sequence
```
百度一番也没解决问题，最后实在没办法，直接使用replace替换掉出错的字符串了。。。
``` python
def vulnSource(vulns):
	for vuln in vulns:
		html = readHtml(vuln[0])
		soup = BeautifulSoup(html,"html.parser")
		vuln_source = soup.select('.article-content a')[0].text
		vuln.append(vuln_source)
		print(vuln)
		time.sleep(0.5) #保险起见，加上延时
		a_vuln = aVuln(vuln[0],vuln[1],vuln[2],vuln[3],vuln[4],vuln[5])
		a_vuln.write2csv()
```
定义此函数补充漏洞列表中的漏洞来源，并顺便把写入的操作也加在这里了。中间的延时是防止触发反爬。
``` python
def getAllPages(url):
	html = readHtml(url)
	soup =BeautifulSoup(html,'html.parser')
	allpage = int(soup.select('.page-link')[-1]['href'][-4:])
	return allpage
```
定义此来获取总页数。

本以为到这里就大功告成，程序看似正常的运行着，没跑多久就突然卡住，报错ConnectionResetError:10054，继续百度之，大概意思是一直发起请求连接未断开触发了反爬，于是乎加上这句：
```
socket.setdefaulttimeout(10)
```
OK，测试通过，截图如下：

![image](http://wx3.sinaimg.cn/mw690/a1a4875fgy1fsvj7q1ih3j218w0jgx6p.jpg)

完整代码见[这里](https://github.com/ekko-zhao/python/tree/master/安全客漏洞列表)
